# 聚类

[toc]

## 什么是聚类

> 聚类是根据在数据中发现的描述对象及其关系的信息，将数据对象分组。目的是，组 内的对象相互之间是相似的（相关的），而不同组中的对象是不同的（不相关的）。 ==组内相似性越大，组间差距越大，说明聚类效果越好。== 
>
> 或者
>
> 聚类就是对大量未知标注的数据集，按数据 的内在相似性将数据集划分为多个类别，使 类别内的数据相似度较大而类别间的数据相 似度较小 
>
> 聚类是 无监督的

## 基本思想

>  给定一个有N个对象的数据集，构造数据的k 个簇，k≤n。满足下列条件：
>
> >  每一个簇至少包含一个对象
> >
> >   每一个对象属于且仅属于一个簇
> >
> >   将满足上述条件的k个簇称作一个合理划分
>
>  对于给定的类别数目k，首先给 出初始划分，通过迭代改变样本和簇的隶属 关系，使得每一次改进之后的划分方案都较 前一次好。

 

## 聚类样本 特征：

> 1. 离散特征
>    - 离散有序特征 eg: 收入级别  --> int
>    - 离散无序特征 eg: 地址 --> string
> 2. 连续特征 eg: 温度、收入  --> float

## 思考

1. 如何达到聚类效果最好？
2. 如何拟定初始聚类中心,拟定几个中心？
3. 既然是无监督，那么需要聚几次？如果是聚类中心小于某个阈值 ，这个 阈值怎么定

## 聚类目的及应用

> ==目的就是 根据抽取出来的特征，将样本id划分到不同的簇（类）内。==
>
> ==来了新数据可以快速归类==
>
> ![image-20200825085251987](Week02-04%E8%81%9A%E7%B1%BB.assets/image-20200825085251987.png)

> 分类，智能分桶
>
> eg：
>
> ![image-20200821184306301](Week02-04%E8%81%9A%E7%B1%BB.assets/image-20200821184306301.png)
>
> 现有根据已有数据聚了100个类，又来了个新用户：
>
> ![image-20200821184401916](Week02-04%E8%81%9A%E7%B1%BB.assets/image-20200821184401916.png)
>
> 去和每个类的聚类中心对比 这个新用户 和 53号类最相似。53簇（类）中有2w个用户。
>
> 判断和这2w个用户哪个最相似呢--> 再聚一个类（如同二分查找（折半查找））

 ### 聚类算法在工业中的应用：

![image-20200820204313571](Week02-04%E8%81%9A%E7%B1%BB.assets/image-20200820204313571.png)

### 用户聚类推荐应用：

![image-20200821190015118](Week02-04%E8%81%9A%E7%B1%BB.assets/image-20200821190015118.png)

## 聚类常用的距离及方法：

> 欧几里得距离、夹角余玄距离
>
> ![image-20200821210650354](Week02-04%E8%81%9A%E7%B1%BB.assets/image-20200821210650354.png)

![image-20200825090410772](Week02-04%E8%81%9A%E7%B1%BB.assets/image-20200825090410772.png)

## 处理离散数据方法：

> 对离散数据做==onehot (独热编码)==
>
> > 使一个离散特征横向展开成多个每个特征只有是(1)或者否(0)并且只有一个特征是1
> >
> > 从一个字段横向展开成多个字段，每个字段只有其中的一个字段被赋予1（激活），其他为0，所以叫做独热编码（onehot）

eg：

> ![image-20200821191113326](Week02-04%E8%81%9A%E7%B1%BB.assets/image-20200821191113326.png)
>
> ![image-20200821191126822](Week02-04%E8%81%9A%E7%B1%BB.assets/image-20200821191126822.png)
>
> 针对对地址、性别特征 u1 对 u2 和 u3 哪个最相似：
>
> 比如用欧几里得距离计算：
> $$
> dis(u1,u2)=\sqrt{(1-0)^2+(0-1)^2+(0-0)^2+(1-1))^2+(0-0)^2}=\sqrt2
> $$
>
> $$
> dis(u1,u3)=\sqrt{(1-1)^2+(0-0)^2+(0-0)^2+(1-1))^2+(0-0)^2}=\sqrt0 = 0
> $$
>
> u1 和 u3 距离为0 最相似

## 处理连续数据的方法

#### 方法一：

##### 	第一种：全部将数据转为离散，onehot后基于cos、欧几里得。

> 等距分桶将其变为离散（聚类然后分桶）然后onehot==推荐==

比如温度：[25到30] 为 1；  [30到35] 为 2

![image-20200821195030709](Week02-04%E8%81%9A%E7%B1%BB.assets/image-20200821195030709.png)

onehot:

![image-20200821195940711](Week02-04%E8%81%9A%E7%B1%BB.assets/image-20200821195940711.png)

##### 第二种：基于距离：离散算离散、连续算连续 最终加权合并两边结果（gower距离）



#### 方法二：

通过模型(embedding)算出 特征字段对应的一串值：eg 地址	{"北京"：[1.3,1.4,1.5,1.5],  "天津"：[0.3,0.4,0.5,1.5]}

embedding(（北京）)=[1.3,1.4,1.5,1.5]

![image-20200821201752687](Week02-04%E8%81%9A%E7%B1%BB.assets/image-20200821201752687.png)

变成

![image-20200821201815840](Week02-04%E8%81%9A%E7%B1%BB.assets/image-20200821201815840.png)

然后把温度也加到地址的集合中,之后再根据距离公式计算。

![image-20200821201858863](Week02-04%E8%81%9A%E7%B1%BB.assets/image-20200821201858863.png)

## 连续数据的数据标准化问题输出：

![image-20200821202128993](Week02-04%E8%81%9A%E7%B1%BB.assets/image-20200821202128993.png)
$$
dis(u1,u2)=(1.7-1.6)^2+(170000-160000)^2
$$

> 非标准化导致的问题：
>
> >  当体重稍微增加或减少一点相对于身高对计算结果影响是巨大的（大数吃小数）。所以需要避免这种情况，就需要对每列做标准化处理
>
> 方法1：max-min
> $$
> \frac{1.7-min(身高)}{max(身高)-min(身高)}
> $$
> 方法2：z-score（原理是拉到正态分布）
> $$
> \frac{1.7-mean(身高)}{\sqrt{var（身高）}}
> $$
> >  tip: mean为均值、var 为方差 

代码：离散数据横向扩展并onehot、连续数据标准化

```python
import pandas as pd
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, minmax_scale, scale

data = {'地址': ['深圳', '北京', '上海'], '体重': [1900, 2500, 7500], '身高': [165, 180, 195], '性别': ['男', '男', '女'], }

df = pd.DataFrame(data)

# 离散无序特征
df_cat = df[['地址', '性别']].copy()
# 离散无序特征横向展开
df_cat_index = pd.DataFrame()
# 连续特征
df_num = df[['体重', '身高']].copy()

l_enc = LabelEncoder()

# 将字符转换为index（将一个离散特征横向展开成多个字段）
for cat in ['地址', '性别']:
    df_cat_index.loc[:, cat] = l_enc.fit_transform(df_cat[cat])

print("离散特征横向展开\r\n", df_cat)

# onehot（离散数据 独热编码）
one_enc = OneHotEncoder()
one_hot = one_enc.fit_transform(df_cat_index)
one_hot_data = one_hot.todense()
print('离散数据 独热编码\r\n', one_hot_data)

# 连续数据标准化
std_data = minmax_scale(df_num)
print('连续数据 minmax 标准化\r\n', std_data)
```

![image-20200825155151265](Week02-04%E8%81%9A%E7%B1%BB.assets/image-20200825155151265.png)

![image-20200825155517877](Week02-04%E8%81%9A%E7%B1%BB.assets/image-20200825155517877.png)

![image-20200825155748432](Week02-04%E8%81%9A%E7%B1%BB.assets/image-20200825155748432.png)

## K-均值聚类（K-means）  

>  K-means算法，也被称为k-平均或k-均值，是一种广泛使用的聚类算法， 或者成为其他聚类算法的基础。
>
>  假定输入样本为$S=x_1,x_2,...,x_m$，则算法步骤为：
>
> >  选择初始的k个类别中心 
> >
> > $$
> > μ_1 μ_2 … μ_k
> > $$
> >
> >
> >  对于每个样本$x_i$，将其标记为距离类别中心最近的类别，即： 
> > $$
> > label_i = arg min||x_i-μ_j|| \\
> > 1\leqq j\leqq k
> > $$
> >
> >
> >  将每个类别中心更新为隶属该类别的所有样本的均值 
> > $$
> > μ_j = \frac{1}{|c_j|}\sum_{i\in c_j}x_i
> > $$
> >  重复最后两步，直到类别中心的变化小于某阈值。
>
>  中止条件： 
>
> >  迭代次数 
> >
> >  簇中心变化率 
> >
> >  最小平方误差MSE(Minimum Squared Error)

#### k-means 注意点

> 1. K-means是初值（聚类中心）敏感的+
>
>    （带来的坏处时什么，当你选择的初始聚类中心距离很近的时候，整个算法的迭代会很慢）
>      会导致，聚类结果差异较大。
>
> 2. 簇内若有异常点，则会导致均值偏离严重，改成求数组中位数更为稳妥 即： K-Mediods聚类（K中值聚类）
>
>   eg: 数组1、2、3、4、100的均值为22，显然距离 “大多数”数据1、2、3、4比较远

##### 二分k-均值聚类（针对初值敏感问题）

> 1. 计算各簇内均方误差，如果某个簇的均方误差特别大，那么簇初值不对。
> 2. 那么把这个类分成两类
> 3. 选择两个聚类中心比较近的合并为一个簇
> 4. 然后再从新聚一次

![image-20200825180314482](Week02-04%E8%81%9A%E7%B1%BB.assets/image-20200825180314482.png)

##### 聚类异常

> 某个簇里面的样本及其稀少它就是异常

##### 轮廓系数（解决K(聚几类)的问题）

> （业界（互联网），如果聚类不是最终、关键的业务捕捉，那么业务会直接拍一个K（300-1000）。打个标签，
> 作为冷启动的一些前置处理）pm。
>  （工业界：相对数据量小，聚类要求精度高，那我们需要合理配置k，服务下游任务）

1. ###### 由pm运营，拍一个

2. ###### canopy 聚类算法初始化聚类

   > 不需要指定k, 只需要指定t1 距离、t2 距离

3. ###### 轮廓系数

> 1. 簇内不相似度：样本i 到簇内其他样本的平均距离$a_i$
>
> 2. 簇间不相似度 : 样本i 到其他各簇的所有样本的平均距离 取 最小值 $b_i$
>
> > $a_i$ 越小说明样本i 越应该被聚到该簇
> >
> > $b_i$ 越大说明样本i越不属于其他簇
>
> 3. 根据样本i 的簇内不相似度$a_i$ 和簇间不相似度$b_i$,定义样本i 的轮廓系数$ s_i$
>
> $$
> s_i=\frac{b_i-a_i}{max(a_i,b_i)}\\
> s_i=
> \begin{cases}
> 1-\frac{a_i}{b_i},  & a_i < b_i \\
> 0, & a_i=b_i \\
> \frac{b_i}{a_i}, & a_i>b_i
> 
> \end{cases}
> $$
>
> > $s_i$接近1，则说明样本i 聚类合理；$s_i$ 接近-1 ，则说明样本i更应该分类到另外的簇； 若$s_i$ 近似为0，则说明样本i在两个簇的边界上
>
> 4. 所有样本的$s_i$ 的均值称为聚类结果的轮廓系数，是该聚类是否紧致的度量。
>    $$
>    SC=\frac{1}{N}\sum_{i=1}^{N}s_i
>    $$
>    

#### K-means聚类方法总结

>  优点：
>
>  >  是解决聚类问题的一种经典算法，简单、快速 、收敛速度快
>  >
>  >  对处理大数据集，该算法保持可伸缩性和高效率 
>  >
>  >  当簇近似为高斯分布时，它的效果较好 
>
>   缺点 
>
>  >  在簇的平均值可被定义的情况下才能使用，可能不适用 于某些应用 
>  >
>  >  必须事先给出k(要生成的簇的数目)，而且对初值敏感， 对于不同的初始值，可能会导致不同结果。 
>  >
>  >  不适合于发现非凸形状的簇或者大小差别很大的簇 
>  >
>  >  对躁声和孤立点数据敏感 
>  >
>  >  采用迭代方法，得到的结果只是局部最优。
>  >
>  >  可作为其他聚类方法的基础算法，如谱聚类

```python
import random
from sklearn import datasets
from sklearn.preprocessing import scale
import numpy as np
import pandas as pd
from sklearn import metrics
# 计算轮廓系数用的

from sklearn.cluster import KMeans


# 归一化数据
def normalize(X, axis=-1, p=2):
    X_scale = scale(X)
    return X_scale


def silhouette_score(X, labels):
    """

    :param X: data
    :param labels:cluster_label
    :return: [-1,1] 越靠近1 聚类结果越好，越靠近-1，聚类结果越差
    """
    score = metrics.silhouette_score(X, labels, metric='cosine')
    return score


class Kmeans():
    """Kmeans聚类算法.
    Parameters:
    -----------
    k: 聚类个数
    max_num_iterations: 最大迭代次数
    Threshold: float

    """

    def __init__(self, k=2, max_num_iterations=500, Threshold=0.0001):
        self.k = k
        self.max_num_iterations = max_num_iterations
        self.Threshold = Threshold

    # 从所有样本中随机选取self.k样本作为初始的聚类中心
    def init_random_centroids(self, X, rand_init=True, init_data=None):
        if rand_init:
            n_samples, n_features = np.shape(X)
            centroids = np.zeros((self.k, n_features))
            for i in range(self.k):
                centroid = X[np.random.choice(range(n_samples))]
                centroids[i] = centroid
        else:
            if isinstance(init_data, np.ndarray):
                centroids = init_data
            elif isinstance(init_data, list):
                centroids = np.array(init_data)
            else:
                raise ('please set right type ')
        return centroids

    def _closest_centroid(self, sample, centroids):
        distances = self.euclidean_distance(sample, centroids)
        closest_i = np.argmin(distances)
        return closest_i

    def euclidean_distance(self, sample, centroids):
        num_features = centroids.shape[1]
        sample = sample.reshape((-1, num_features))
        centroids = centroids.reshape((-1, num_features))
        dis = np.sum(np.power(centroids - sample, 2), axis=1)
        return dis

    def create_clusters(self, centroids, X):
        n_samples = np.shape(X)[0]
        # 一轮 判断 将每个样本的id，打到不同簇内
        clusters = [[] for _ in range(self.k)]
        for sample_i, sample in enumerate(X):
            # 该方法，判断一个样本距离哪个聚类中心最近，返回其聚类编号
            centroid_i = self._closest_centroid(sample, centroids)
            clusters[centroid_i].append(sample_i)
        return clusters

    # 对中心进行更新
    def update_centroids(self, clusters, X):
        n_features = np.shape(X)[1]
        centroids = np.zeros((self.k, n_features))
        for i, cluster in enumerate(clusters):
            centroid = np.mean(X[cluster], axis=0)
            centroids[i] = centroid
        return centroids

    # 将所有样本进行归类，其所在的类别的索引就是其类别标签
    def get_cluster_labels(self, clusters, X):
        y_pred = np.zeros(np.shape(X)[0])
        for cluster_i, cluster in enumerate(clusters):
            for sample_i in cluster:
                y_pred[sample_i] = cluster_i
        return y_pred

    # 对整个数据集X进行Kmeans聚类，返回其聚类的标签
    def main(self, X, rand_init=True, init_data=None):
        # step1  初始化聚类中心。从所有样本中随机选取self.k样本作为初始的聚类中心
        centroids = self.init_random_centroids(X, rand_init, init_data)

        # step2 迭代，直到算法收敛(上一次的聚类中心和这一次的聚类中心几乎重合)或者达到最大迭代次数
        for _ in range(self.max_num_iterations):
            # 计算所有的数据距离类的距离，距离近的归到该类
            clusters = self.create_clusters(centroids, X)
            # 将上一步的聚类中心，作为老的聚类中心
            former_centroids = centroids

            # 计算新的聚类中心
            centroids = self.update_centroids(clusters, X)
            self.Cluster_center = centroids
            # 如果聚类中心几乎没有变化，说明算法已经收敛，退出迭代
            diff = centroids - former_centroids
            if diff.any() < self.Threshold:
                break

        return self.get_cluster_labels(clusters, X)


if __name__ == "__main__":
    # make_data
    X, y = datasets.make_blobs(n_samples=10000,
                               n_features=3,
                               centers=[[3, 3, 3], [0, 0, 0], [1, 1, 1], [2, 2, 2]],
                               cluster_std=[0.2, 0.1, 0.2, 0.2],
                               random_state=9)
    #

    # 用 自研 Kmeans算法进行聚类
    clf1 = Kmeans(k=4)
    clf2 = KMeans(4)

    # centers = [[3, 3, 3], [0, 0, 0], [1, 1, 1], [2, 2, 2]]
    # s=clf1.predict(X,rand_init=False,init_data=centers)
    #
    # print(s)

    y_pred_type1 = clf1.main(X)
    X = pd.DataFrame(X)
    y_pred_type2 = pd.DataFrame(clf2.fit_predict(X))

    re = pd.concat([X, y_pred_type2], axis=1)
    print(re.head(100))

    print(silhouette_score(X, y_pred_type1))
    print(silhouette_score(X, y_pred_type2))

    import copy

    s1 = [1, 3, 4, 5, 5]
    s2 = [1, 2, 3, 4, 5]
    s3 = [1, 3, 4, 5, 5]

    diff = np.array(s1) - np.array(s2)


```

